import numpy as np
import torch
import numpy.typing as npt

from cs336_basics.tokenizer import Tokenizer
from llm.ckpt import save_checkpoint
from llm.optimizer import AdamW, cross_entropy_loss
from llm.transformer import LLM


vocab_size = 50256 + 1
context_length = 128
d_model = 512
d_ff = 1314
theta = 10_000
num_layers = 4
num_heads = 16
batch_size = 8
n_epoch = 100
device = "cuda"


def get_batch(
    dataset: npt.NDArray, batch_size: int, context_length: int, device: str
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Given a dataset (a 1D numpy array of integers) and a desired batch size and
    context length, sample language modeling input sequences and their corresponding
    labels from the dataset.

    Args:
        dataset (np.array): 1D numpy array of integer token IDs in the dataset.
        batch_size (int): Desired batch size to sample.
        context_length (int): Desired context length of each sampled example.
        device (str): PyTorch device string (e.g., 'cpu' or 'cuda:0') indicating the device
            to place the sampled input sequences and labels on.

    Returns:
        Tuple of torch.LongTensors of shape (batch_size, context_length). The first tuple item
        is the sampled input sequences, and the second tuple item is the corresponding
        language modeling labels.
    """
    n = len(dataset)
    m = context_length
    if n <= m:
        raise ValueError(f"Sequence length n={n} must be greater than context_length={m}")

    # æœ€å¤§å¯é‡‡æ ·èµ·å§‹ä½ç½®æ˜¯ n - m - 1 (inclusive)
    # numpy.randint(low, high) å–å€¼èŒƒå›´æ˜¯ [low, high)
    starts = np.random.randint(0, n - m, size=batch_size)  # shape (B,)

    # æ„é€ ç´¢å¼•çŸ©é˜µï¼šæ¯ä¸€è¡Œæ˜¯ start + [0,1,...,m-1]
    offsets = np.arange(m)  # shape (m,)
    idx = starts[:, None] + offsets[None, :]  # shape (B, m)

    # inputs: dataset[idx] -> shape (B, m)
    inputs_np = dataset[idx]

    # targets: shift by 1 -> indices idx + 1
    targets_np = dataset[idx + 1]

    # è½¬ä¸º torch tensors æ”¾åˆ° deviceï¼ˆlong ç±»å‹ï¼‰
    inputs = torch.from_numpy(inputs_np).to(device=device, dtype=torch.long)
    targets = torch.from_numpy(targets_np).to(device=device, dtype=torch.long)

    return inputs, targets


def tokenize():
    tokenizer = Tokenizer.from_files(
        vocab_filepath='tests/fixtures/gpt2_vocab.json',
        merges_filepath='tests/fixtures/gpt2_merges.txt',
        special_tokens=["<|endoftext|>"]
    )
    tokenizer.use_gpt_mapping = True

    # vocabs = list(tokenizer.vocab.items())
    # merges = list(tokenizer.merges)
    # print(f"{vocabs[0]=}, {vocabs[1]=}, {vocabs[2]=}")
    # print(f"{merges[0]=}, {merges[1]=}, {merges[2]=}")

    # read from tests/fixtures/corpus.en and tokenize
    with open('tests/fixtures/tinystories_sample_5M.txt', 'r', encoding='utf-8') as f:
        text = f.read()

    tokens = tokenizer.encode(text)
    np.save("tokens_tinystories_sample_5M.npy", tokens)

def train():
    llm = LLM(
        vocab_size=vocab_size,
        context_length=context_length,
        d_model=d_model,
        num_layers=num_layers,
        num_heads=num_heads,
        d_ff=d_ff,
        rope_theta=theta,
        device=torch.device(device),
        dtype=torch.float32
    )

    optimizer = AdamW(params=llm.parameters())
    tokens = np.load("tokens_tinystories_sample_5M.npy", mmap_mode='r')

    # Dummy training loop
    for epoch in range(n_epoch):
        inputs, outputs = get_batch(
            dataset=tokens,
            batch_size=batch_size,
            context_length=context_length,
            device=device
        )
        optimizer.zero_grad()
        logits = llm(inputs)
        loss = cross_entropy_loss(logits.view(-1, vocab_size), outputs.view(-1))
        loss.backward()
        optimizer.step()

        print(f"Epoch {epoch + 1}, Loss: {loss.cpu().item()}")

        save_checkpoint(llm, optimizer, epoch + 1, f"checkpoint_epoch_{epoch + 1}.pt")

def test_fast():
    tokenizer = Tokenizer.from_files(
        vocab_filepath='tests/fixtures/gpt2_vocab.json',
        merges_filepath='tests/fixtures/gpt2_merges.txt',
        special_tokens=["<|endoftext|>"],
    )
    tokenizer.use_gpt_mapping = True

    tests = [
        "hello world",
        # "è¿™æ˜¯ä¸­æ–‡",
        "some text that i'll pre-tokenize",
        # "ğŸ™‚ emoji test",
        # "cafÃ© naÃ¯ve",
        "some rare char"
    ]

    for t in tests:
        ids = tokenizer.encode(t)
        print(ids)
        toks = tokenizer.decode(ids)
        # æ£€æŸ¥æ¯ä¸ª token æ˜¯å¦åœ¨ vocab å­—å…¸ä¸­ï¼ˆç†è®ºä¸Šéƒ½åº”è¯¥åœ¨ï¼‰
        print("TEXT:", t)
        print("TOKENS:", toks)
        print("---")

def read_npy_basic(filepath):
    """è¯»å–npyæ–‡ä»¶å¹¶æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯"""
    try:
        data = np.load(filepath)
        
        print("=== NPY æ–‡ä»¶ä¿¡æ¯ ===")
        print(f"æ–‡ä»¶è·¯å¾„: {filepath}")
        print(f"æ•°æ®å½¢çŠ¶: {data.shape}")
        print(f"æ•°æ®ç»´åº¦: {data.ndim}")
        print(f"æ•°æ®ç±»å‹: {data.dtype}")
        print(f"æ•°æ®å¤§å°: {data.size}")
        print(f"å­—èŠ‚å¤§å°: {data.nbytes} bytes")
        print(f"å†…å­˜å¸ƒå±€: {data.flags}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªå…ƒç´ 
        print(f"\nå‰5ä¸ªå…ƒç´ : {data.flat[:5] if data.size > 5 else data.flat[:]}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if np.issubdtype(data.dtype, np.number):
            print(f"æœ€å°å€¼: {data.min():.4f}")
            print(f"æœ€å¤§å€¼: {data.max():.4f}")
            print(f"å¹³å‡å€¼: {data.mean():.4f}")
            print(f"æ ‡å‡†å·®: {data.std():.4f}")
        
        return data
        
    except FileNotFoundError:
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {filepath}")
        return None
    except Exception as e:
        print(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None


if __name__ == "__main__":
    # train()
    # test_fast()
    # read_npy_basic("tokens.npy")
    # prompt = input("Input a prompt:")
    # inference(prompt)
    train()
